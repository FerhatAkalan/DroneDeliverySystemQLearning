# -*- coding: utf-8 -*-
"""
Paket DaÄŸÄ±tÄ±m DronlarÄ± SimÃ¼latÃ¶rÃ¼
Vize Projesi - Ferhat

Senaryo: Dronlar, ÅŸehir iÃ§i teslimatlarda birden Ã§ok noktaya en verimli ÅŸekilde paket gÃ¶tÃ¼rmeli.
PekiÅŸtirmeli Ã–ÄŸrenme ile YaklaÅŸÄ±m (Taxi-v3 benzeri):
- Grid tabanlÄ± ortam (Ã¶r: 5x5)
- Q-Learning ile Ã¶ÄŸrenme
- PyQt5 arayÃ¼zÃ¼
- EÄŸitim ve simÃ¼lasyon hÄ±zlarÄ± ayarlanabilir
- Q-Table kaydet/yÃ¼kle
- Batarya, kargo, teslimat noktalarÄ±, take off/landing animasyonlarÄ±
"""

import sys
import os
import random
import pickle
import numpy as np
from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QPushButton, QVBoxLayout, QHBoxLayout, QLabel, QSpinBox, QDoubleSpinBox, QGroupBox, QGridLayout, QFileDialog, QMessageBox, QComboBox, QSlider)
from PyQt5.QtCore import Qt, QTimer, pyqtSignal, QThread
from PyQt5.QtGui import QPainter, QColor, QBrush, QPen, QFont, QIcon, QPixmap

# =====================
# Ortam (Environment) SÄ±nÄ±fÄ±
# =====================
class DroneDeliveryEnv:
    """
    Grid tabanlÄ± ÅŸehir ortamÄ± (Taxi-v3 benzeri):
    - YeÅŸil: Kargo deposu
    - KÄ±rmÄ±zÄ±: Teslimat noktalarÄ±
    - Mavi: Drone
    - Batarya, kargo, teslimatlar, uÃ§uÅŸ durumu
    """
    def __init__(self, grid_size=5, max_steps=100, n_deliveries=1):
        # OrtamÄ±n temel parametreleri: grid boyutu, maksimum adÄ±m sayÄ±sÄ±, teslimat noktasÄ± sayÄ±sÄ±
        self.grid_size = grid_size
        self.max_steps = max_steps
        self.n_deliveries = n_deliveries
        # Eylem uzayÄ±: 
        # 0: AÅŸaÄŸÄ±, 1: SaÄŸa, 2: YukarÄ±, 3: Sola, 4: Kargo Al/BÄ±rak, 5: Kalk/Ä°n
        self.action_space_n = 6  # Drone'un yapabileceÄŸi toplam eylem sayÄ±sÄ±
        #4 sabit teslimat noktasÄ± (kÃ¶ÅŸeler)
        self.fixed_delivery_points = [
            np.array([0, 0]),
            np.array([0, self.grid_size-1]),
            np.array([self.grid_size-1, 0]),
            np.array([self.grid_size-1, self.grid_size-1])
        ]
        # Batarya tÃ¼ketim oranlarÄ± (her hareket/kalkÄ±ÅŸ/iniÅŸ iÃ§in)
        self.move_battery_cost = 1  # Normal hareket baÅŸÄ±na batarya tÃ¼ketimi
        self.takeoff_battery_cost = 5  # KalkÄ±ÅŸ iÃ§in batarya tÃ¼ketimi
        self.landing_battery_cost = 5  # Ä°niÅŸ iÃ§in batarya tÃ¼ketimi
        
        # OrtamÄ± baÅŸlat
        self.reset()

    def reset(self):
        # OrtamÄ± baÅŸlangÄ±Ã§ durumuna sÄ±fÄ±rlar. Her yeni bÃ¶lÃ¼m (episode) baÅŸÄ±nda Ã§aÄŸrÄ±lÄ±r.
        # Drone'u grid Ã¼zerinde rastgele bir konumda baÅŸlat (Taxi-v3 mantÄ±ÄŸÄ±)
        self.drone_pos = np.array([
            random.randint(0, self.grid_size-1),
            random.randint(0, self.grid_size-1)
        ])
        # Kargo deposunun konumu sabit.
        self.cargo_depot_pos = np.array([self.grid_size-1, self.grid_size-1])
        # Teslimat noktasÄ± sayÄ±sÄ±nÄ± her episode'da 1-3 arasÄ± rastgele seÃ§
        self.n_deliveries = random.randint(1, 3)
        # Kargo deposu kÃ¶ÅŸesini hariÃ§ tutarak teslimat noktasÄ± seÃ§ (Taxi-v3 mantÄ±ÄŸÄ±)
        # Teslimat noktalarÄ±, kargo deposu olmayan kÃ¶ÅŸelerden rastgele seÃ§ilir.
        available_indices = [i for i in range(len(self.fixed_delivery_points)) if not np.array_equal(self.fixed_delivery_points[i], self.cargo_depot_pos)]
        chosen_indices = random.sample(available_indices, self.n_deliveries)
        self.delivery_points = [self.fixed_delivery_points[i].copy() for i in chosen_indices]
        self.delivery_indices = chosen_indices  # State iÃ§in indexler
        # Drone'un baÅŸlangÄ±Ã§ durumu: kargo yok, batarya dolu, adÄ±m sayÄ±sÄ± sÄ±fÄ±r, teslimatlar yapÄ±lmamÄ±ÅŸ.
        self.has_cargo = False
        self.battery = 100
        self.steps = 0
        self.delivered = [False]*len(self.delivery_points)
        self.done = False # BÃ¶lÃ¼mÃ¼n bitip bitmediÄŸini gÃ¶sterir.
        self.is_flying = False # Drone'un uÃ§uÅŸ durumu.
        self.landing_state = "landed" # Ä°niÅŸ/kalkÄ±ÅŸ animasyon durumu.
        self.landing_animation_step = 0 # Ä°niÅŸ/kalkÄ±ÅŸ animasyon adÄ±mÄ±.
        self.last_reward = 0  # Son adÄ±mda alÄ±nan Ã¶dÃ¼l
        self.total_reward = 0  # Toplam Ã¶dÃ¼l (her episode baÅŸÄ±nda sÄ±fÄ±rlanÄ±r)
        return self.get_state() # OrtamÄ±n mevcut durumunu dÃ¶ndÃ¼rÃ¼r.

    def get_state(self):
        # OrtamÄ±n mevcut durumunu temsil eden bir tuple dÃ¶ndÃ¼rÃ¼r.
        # Bu durum, Q-tablosunda anahtar olarak kullanÄ±lÄ±r.
        x, y = self.drone_pos
        state = (x, y, int(self.has_cargo), int(self.is_flying))
        for d in self.delivered:
            state += (int(d),)
        # Batarya seviyesi, state'e DAHA HASSAS dahil edilir (0-10 arasÄ± discretize)
        battery_level = min(int(self.battery / 10), 10)
        state += (battery_level,)
        # Teslimat noktasÄ± indexlerini state'e ekle
        for idx in self.delivery_indices:
            state += (idx,)
        return state  # ArtÄ±k hash ve mod yok, doÄŸrudan tuple kullanÄ±lÄ±yor.

    def step(self, action):
        """
        Drone'a verilen eylemi uygular ve ortamÄ± bir adÄ±m ilerletir.
        Args:
            action (int):
                0: AÅŸaÄŸÄ±, 1: SaÄŸa, 2: YukarÄ±, 3: Sola
                4: Kargo Al/BÄ±rak (Yerdeyken kargo al veya teslim et)
                5: Kalk/Ä°n (Take off/landing, uÃ§uÅŸ durumunu deÄŸiÅŸtirir)
        Returns:
            tuple: (next_state, reward, done, info)
                next_state: Yeni durumun hashlenmiÅŸ temsili
                reward: Bu adÄ±mda alÄ±nan Ã¶dÃ¼l/ceza
                done: Senaryo tamamlandÄ± mÄ±?
                info: Ek bilgi (Ã¶r. neden bitti, hangi eylem yapÄ±ldÄ±)
        Drone kargo almak ve bÄ±rakmak iÃ§in landing durumunda olmalÄ±. Havadayken kargo bÄ±rakÄ±lamaz alÄ±namaz.
        """
        if self.done:
            return self.get_state(), 0, True, {"info": "Senaryo zaten tamamlanmÄ±ÅŸ."}
        
        # BaÅŸlangÄ±Ã§ durumu
        old_pos = self.drone_pos.copy()
        reward = 0
        info = {}
        action_emojis = {
            0: 'â¬‡ï¸',  # AÅŸaÄŸÄ±
            1: 'â¡ï¸',  # SaÄŸa
            2: 'â¬†ï¸',  # YukarÄ±
            3: 'â¬…ï¸',  # Sola
            4: 'ğŸ“¦',  # Kargo Al/BÄ±rak
            5: 'ğŸ›«/ğŸ›¬',  # Kalk/Ä°n
        }
        action_names = {
            0: 'AÅŸaÄŸÄ± hareket',
            1: 'SaÄŸa hareket',
            2: 'YukarÄ± hareket',
            3: 'Sola hareket',
            4: 'Kargo Al/BÄ±rak',
            5: 'Kalk/Ä°n',
        }
        # --- Eylem tipine gÃ¶re Ã¶dÃ¼l/ceza ---
        if action <= 3:  # Hareket eylemleri
            if not self.is_flying:
                reward -= 2
                info["action"] = f"{action_emojis[action]} {action_names[action]} (action={action}) | Drone yerdeyken hareket edemez! Ã–nce kalkÄ±ÅŸ yapÄ±n."
            else:
                if action == 0:
                    self.drone_pos[0] = min(self.drone_pos[0] + 1, self.grid_size - 1)
                elif action == 1:
                    self.drone_pos[1] = min(self.drone_pos[1] + 1, self.grid_size - 1)
                elif action == 2:
                    self.drone_pos[0] = max(self.drone_pos[0] - 1, 0)
                elif action == 3:
                    self.drone_pos[1] = max(self.drone_pos[1] - 1, 0)
                if np.array_equal(old_pos, self.drone_pos):
                    reward -= 5
                    info["action"] = f"{action_emojis[action]} {action_names[action]} (action={action}) | Hareket edilemedi."
                else:
                    reward -= 1
                    self.battery -= self.move_battery_cost
                    info["action"] = f"{action_emojis[action]} {action_names[action]} (action={action})"
        elif action == 4:  # Kargo Al/BÄ±rak
            if self.is_flying:
                reward -= 10
                info["action"] = f"{action_emojis[action]} {action_names[action]} (action={action}) | Drone havadayken kargo alÄ±namaz/bÄ±rakÄ±lamaz! Ã–nce iniÅŸ yapÄ±n."
            else:
                if np.array_equal(self.drone_pos, self.cargo_depot_pos) and not self.has_cargo:
                    self.has_cargo = True
                    reward += 50
                    info["action"] = f"{action_emojis[action]} Kargo alÄ±ndÄ± (action={action})"
                elif self.has_cargo:
                    delivered_any = False
                    for i, delivery_point in enumerate(self.delivery_points):
                        if np.array_equal(self.drone_pos, delivery_point) and not self.delivered[i]:
                            self.delivered[i] = True
                            self.has_cargo = False
                            reward += 200
                            info["action"] = f"{action_emojis[action]} {i+1}. teslimat tamamlandÄ± (action={action})"
                            delivered_any = True
                            break
                    if not delivered_any:
                        reward -= 30
                        info["action"] = f"{action_emojis[action]} YanlÄ±ÅŸ yerde teslimat (action={action})"
                else:
                    reward -= 30
                    info["action"] = f"{action_emojis[action]} Burada kargo alÄ±namaz/bÄ±rakÄ±lamaz (action={action})"
        elif action == 5:  # Kalk/Ä°n
            if not self.is_flying:
                self.is_flying = True
                self.landing_state = "taking_off"
                self.landing_animation_step = 0
                reward -= 3
                info["action"] = f"ğŸ›« KalkÄ±ÅŸ (action={action})"
                self.battery -= self.takeoff_battery_cost
            else:
                self.is_flying = False
                self.landing_state = "landing"
                self.landing_animation_step = 0
                reward -= 3
                info["action"] = f"ğŸ›¬ Ä°niÅŸ (action={action})"
                self.battery -= self.landing_battery_cost

        # --- Hedefe yaklaÅŸma/uzaklaÅŸma Ã¶dÃ¼l/ceza ---
        target_pos = None
        if not self.has_cargo and not all(self.delivered):
            target_pos = self.cargo_depot_pos
        elif self.has_cargo:
            min_dist = float('inf')
            for i, point in enumerate(self.delivery_points):
                if not self.delivered[i]:
                    dist = np.sum(np.abs(self.drone_pos - point))
                    if dist < min_dist:
                        min_dist = dist
                        target_pos = point
        if target_pos is not None:
            old_dist = np.sum(np.abs(old_pos - target_pos))
            new_dist = np.sum(np.abs(self.drone_pos - target_pos))
            if self.is_flying and new_dist < old_dist:
                reward += 5  # Hedefe yaklaÅŸma Ã¶dÃ¼lÃ¼
            elif self.is_flying and new_dist > old_dist:
                reward -= 2  # Hedeften uzaklaÅŸma cezasÄ±
            if np.array_equal(self.drone_pos, target_pos):
                if not self.is_flying and action == 4:
                    reward += 10  # DoÄŸru yerde doÄŸru eylem bonusu
                elif self.is_flying and action == 5:
                    reward += 5  # DoÄŸru yerde iniÅŸ bonusu

        # --- Batarya kontrolÃ¼ ---
        if self.battery <= 0:
            reward -= 100  # Batarya biterse aÄŸÄ±r ceza
            self.battery = 0
            self.done = True
            info["done_reason"] = "Batarya bitti"

        # --- AdÄ±m sÄ±nÄ±rÄ± ---
        self.steps += 1
        if self.steps >= self.max_steps:
            reward -= 50  # Maksimum adÄ±m cezasÄ±
            self.done = True
            info["done_reason"] = "Maksimum adÄ±m sayÄ±sÄ±na ulaÅŸÄ±ldÄ±"

        # --- TÃ¼m teslimatlar tamamlandÄ±ysa ---
        if all(self.delivered):
            remaining_battery_bonus = self.battery
            reward += 200 + remaining_battery_bonus  # BÃ¼yÃ¼k Ã¶dÃ¼l ve kalan batarya bonusu
            self.done = True
            info["done_reason"] = f"TÃ¼m teslimatlar tamamlandÄ±! Kalan batarya: %{self.battery}"

        # Ä°niÅŸ/kalkÄ±ÅŸ animasyon durumlarÄ±nÄ± gÃ¼ncelle
        # Bu adÄ±mlar, gÃ¶rsel arayÃ¼zde animasyonun dÃ¼zgÃ¼n Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.
        if self.landing_state == "taking_off":
            self.landing_animation_step += 1
            if self.landing_animation_step >= 3:  # 3 adÄ±mda tamamlanan kalkÄ±ÅŸ animasyonu
                self.landing_state = "flying"
        elif self.landing_state == "landing":
            self.landing_animation_step += 1
            if self.landing_animation_step >= 3:  # 3 adÄ±mda tamamlanan iniÅŸ animasyonu
                self.landing_state = "landed"
        
        self.last_reward = reward  # Son Ã¶dÃ¼l bilgisini gÃ¼ncelle
        self.total_reward += reward  # Toplam Ã¶dÃ¼lÃ¼ gÃ¼ncelle
        # Son aksiyon bilgisini ortamda sakla
        self.last_action_info = info.get("action", "-")
        return self.get_state(), reward, self.done, info # Yeni durum, Ã¶dÃ¼l, bÃ¶lÃ¼m durumu ve ek bilgiyi dÃ¶ndÃ¼r.
# =====================
# Q-Learning AjanÄ±
# =====================
class QLearningAgent:
    """
    Q-Learning ajanÄ±: Epsilon-greedy, Q-Table, deneyim havuzu
    """
    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):
        # Q-Learning parametreleri ve Q-Table baÅŸlatma
        self.env = env # AjanÄ±n etkileÅŸimde bulunacaÄŸÄ± ortam.
        self.alpha = alpha  # Ã–ÄŸrenme oranÄ± (learning rate): Yeni bilginin ne kadar dikkate alÄ±nacaÄŸÄ±nÄ± belirler.
        self.gamma = gamma  # Ä°skonto faktÃ¶rÃ¼ (discount factor): Gelecekteki Ã¶dÃ¼llerin bugÃ¼nkÃ¼ deÄŸerini belirler.
        self.epsilon = epsilon  # KeÅŸif oranÄ± (exploration rate): AjanÄ±n ne sÄ±klÄ±kla rastgele eylem seÃ§eceÄŸini belirler.
        self.epsilon_decay = epsilon_decay  # Epsilon azalma oranÄ±: Epsilon'un her bÃ¶lÃ¼m sonunda ne kadar azalacaÄŸÄ±nÄ± belirler.
        self.min_epsilon = min_epsilon  # Minimum keÅŸif oranÄ±: Epsilon'un dÃ¼ÅŸebileceÄŸi en dÃ¼ÅŸÃ¼k deÄŸer.
        self.q_table = {}  # Q-Tablosu (durum-aksiyon deÄŸerleri): Her durum-eylem Ã§ifti iÃ§in beklenen Ã¶dÃ¼lÃ¼ saklar.
        self.experience_buffer = []  # Deneyim havuzu (replay buffer): AjanÄ±n geÃ§miÅŸ deneyimlerini saklar.
        self.buffer_size = 1000 # Deneyim havuzunun maksimum boyutu.
        self.batch_size = 32 # Deneyim tekrarÄ± sÄ±rasÄ±nda kullanÄ±lacak Ã¶rneklem boyutu.
        self.learn_interval = 4 # KaÃ§ adÄ±mda bir deneyim tekrarÄ± yapÄ±lacaÄŸÄ±.
        self.step_counter = 0 # AdÄ±m sayacÄ±.

    def get_q_value(self, state, action):
        # Belirli bir durum ve aksiyon iÃ§in Q-deÄŸerini dÃ¶ndÃ¼r
        # EÄŸer durum Q-tablosunda yoksa, o durum iÃ§in tÃ¼m eylemlerin Q-deÄŸerlerini sÄ±fÄ±r olarak baÅŸlatÄ±r.
        if state not in self.q_table:
            self.q_table[state] = np.zeros(self.env.action_space_n)
        return self.q_table[state][action]

    def select_action(self, state, training=True):
        # Epsilon-greedy aksiyon seÃ§imi
        # EÄŸitim modunda ve rastgele bir sayÄ± epsilon'dan kÃ¼Ã§Ã¼kse, rastgele bir eylem seÃ§ilir (keÅŸif).
        # Aksi takdirde, mevcut durum iÃ§in en yÃ¼ksek Q-deÄŸerine sahip eylem seÃ§ilir (sÃ¶mÃ¼rÃ¼).
        if training and np.random.rand() < self.epsilon:
            return np.random.randint(self.env.action_space_n)  # Rastgele aksiyon (keÅŸif)
        else:
            if state not in self.q_table: # EÄŸer durum Q-tablosunda yoksa, baÅŸlat.
                self.q_table[state] = np.zeros(self.env.action_space_n)
            max_value = np.max(self.q_table[state]) # En yÃ¼ksek Q-deÄŸerini bul.
            # En yÃ¼ksek Q-deÄŸerine sahip birden fazla eylem varsa, aralarÄ±ndan rastgele birini seÃ§.
            max_indices = np.where(self.q_table[state] == max_value)[0]
            return np.random.choice(max_indices)  # En iyi aksiyon (sÃ¶mÃ¼rÃ¼)

    def learn(self, state, action, reward, next_state, done):
        # Q-Table gÃ¼ncellemesi ve deneyim havuzuna ekleme
        # Bu fonksiyon, ajanÄ±n bir eylem gerÃ§ekleÅŸtirdikten sonra Q-tablosunu gÃ¼ncellemesini saÄŸlar.
        self.add_experience(state, action, reward, next_state, done) # Deneyimi havuza ekle.
        if state not in self.q_table: # Durum Q-tablosunda yoksa baÅŸlat.
            self.q_table[state] = np.zeros(self.env.action_space_n)
        if next_state not in self.q_table: # Sonraki durum Q-tablosunda yoksa baÅŸlat.
            self.q_table[next_state] = np.zeros(self.env.action_space_n)
        
        current_q = self.q_table[state][action] # Mevcut Q-deÄŸeri.
        # EÄŸer bÃ¶lÃ¼m bittiyse (done=True), gelecekteki maksimum Q-deÄŸeri 0 olur.
        # Aksi takdirde, sonraki durum iÃ§in maksimum Q-deÄŸeri alÄ±nÄ±r.
        max_future_q = 0 if done else np.max(self.q_table[next_state])
        # Q-deÄŸeri gÃ¼ncelleme formÃ¼lÃ¼ (Bellman denklemi).
        new_q = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)
        self.q_table[state][action] = new_q # Q-tablosunu gÃ¼ncelle.
        
        self.step_counter += 1
        # Deneyim tekrarÄ±nÄ± belirli aralÄ±klarla uygula
        # Deneyim havuzu yeterince doluysa ve belirli bir adÄ±m aralÄ±ÄŸÄ±na ulaÅŸÄ±ldÄ±ysa deneyim tekrarÄ± yapÄ±lÄ±r.
        if self.step_counter % self.learn_interval == 0 and len(self.experience_buffer) >= self.batch_size:
            self.experience_replay()

    def add_experience(self, state, action, reward, next_state, done):
        # Deneyim havuzuna yeni deneyim ekle
        # EÄŸer deneyim havuzu doluysa, en eski deneyim silinir.
        if len(self.experience_buffer) >= self.buffer_size:
            self.experience_buffer.pop(0)
        self.experience_buffer.append((state, action, reward, next_state, done)) # Yeni deneyimi ekle.

    def experience_replay(self):
        # Deneyim havuzundan rastgele Ã¶rneklerle Ã¶ÄŸrenme
        # Bu, ajanÄ±n geÃ§miÅŸ deneyimlerinden tekrar Ã¶ÄŸrenmesini saÄŸlayarak Ã¶ÄŸrenmeyi daha stabil hale getirir.
        batch = random.sample(self.experience_buffer, self.batch_size) # Havuzdan rastgele bir batch seÃ§.
        for state, action, reward, next_state, done in batch: # SeÃ§ilen her deneyim iÃ§in Q-deÄŸerini gÃ¼ncelle.
            if state not in self.q_table:
                self.q_table[state] = np.zeros(self.env.action_space_n)
            if next_state not in self.q_table:
                self.q_table[next_state] = np.zeros(self.env.action_space_n)
            current_q = self.q_table[state][action]
            max_future_q = 0 if done else np.max(self.q_table[next_state])
            replay_alpha = self.alpha * 0.7 # Deneyim tekrarÄ± iÃ§in biraz daha dÃ¼ÅŸÃ¼k bir Ã¶ÄŸrenme oranÄ± kullanÄ±labilir.
            new_q = current_q + replay_alpha * (reward + self.gamma * max_future_q - current_q)
            self.q_table[state][action] = new_q

    def decay_epsilon(self):
        # Epsilon'u kademeli olarak azalt
        # Bu, ajanÄ±n zamanla daha fazla sÃ¶mÃ¼rÃ¼ yapmasÄ±nÄ± ve daha az keÅŸif yapmasÄ±nÄ± saÄŸlar.
        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)

    def save_q_table(self, filename):
        # Q-Tablosunu dosyaya kaydet
        # EÄŸitimli modelin daha sonra kullanÄ±labilmesi iÃ§in Q-tablosu kaydedilir.
        with open(filename, 'wb') as f:
            pickle.dump(self.q_table, f)

    def load_q_table(self, filename):
        # Q-Tablosunu dosyadan yÃ¼kle
        # Daha Ã¶nce eÄŸitilmiÅŸ bir modelin Q-tablosu yÃ¼klenir.
        with open(filename, 'rb') as f:
            self.q_table = pickle.load(f)

# =====================
# EÄŸitim Thread'i (PyQt5)
# =====================
class TrainingThread(QThread): # PyQt5 QThread sÄ±nÄ±fÄ±ndan miras alÄ±r, bÃ¶ylece arayÃ¼z donmadan eÄŸitim yapÄ±labilir.
    progress = pyqtSignal(int, float, float, float)  # episode, reward, steps, epsilon -> EÄŸitim ilerlemesini bildiren sinyal.
    finished = pyqtSignal(list, list) # EÄŸitim bittiÄŸinde Ã¶dÃ¼l ve adÄ±m listelerini gÃ¶nderen sinyal.
    state_update = pyqtSignal() # Ortam durumunun gÃ¼ncellenmesi gerektiÄŸini bildiren sinyal (gÃ¶rsel arayÃ¼z iÃ§in).
    def __init__(self, env, agent, episodes, update_interval=10, mode="fast", delay=0.1): # "ansi" -> "fast"
        super().__init__()
        self.env = env # EÄŸitim ortamÄ±.
        self.agent = agent # EÄŸitilecek ajan.
        self.episodes = episodes # Toplam eÄŸitim bÃ¶lÃ¼mÃ¼ sayÄ±sÄ±.
        self.running = True # EÄŸitimin devam edip etmediÄŸini kontrol eden bayrak.
        self.update_interval = update_interval # fast modunda ne sÄ±klÄ±kta arayÃ¼zÃ¼n gÃ¼ncelleneceÄŸi.
        self.mode = mode  # 'human' (canlÄ± izleme) veya 'fast' (hÄ±zlÄ± eÄŸitim).
        self.delay = delay  # 'human' modunda adÄ±mlar arasÄ± gecikme (saniye).
    def run(self):
        # EÄŸitim dÃ¶ngÃ¼sÃ¼ (her episode iÃ§in)
        rewards_per_episode = [] # Her bÃ¶lÃ¼mdeki toplam Ã¶dÃ¼lÃ¼ saklar.
        steps_per_episode = [] # Her bÃ¶lÃ¼mdeki adÄ±m sayÄ±sÄ±nÄ± saklar.
        for episode in range(self.episodes):
            if not self.running: # EÄŸer durdurma sinyali geldiyse eÄŸitimi sonlandÄ±r.
                break
            state = self.env.reset() # OrtamÄ± sÄ±fÄ±rla.
            total_reward = 0 # Bu bÃ¶lÃ¼mdeki toplam Ã¶dÃ¼l.
            done = False # BÃ¶lÃ¼mÃ¼n bitip bitmediÄŸi.
            step_counter = 0 # Bu bÃ¶lÃ¼mdeki adÄ±m sayÄ±sÄ±.
            self.state_update.emit() # ArayÃ¼zÃ¼ gÃ¼ncelle.
            while not done and self.running: # BÃ¶lÃ¼m bitene kadar veya durdurma sinyali gelene kadar devam et.
                action = self.agent.select_action(state, training=True) # Ajan bir eylem seÃ§er.
                next_state, reward, done, info = self.env.step(action) # Ortamda eylemi uygula.
                self.agent.learn(state, action, reward, next_state, done) # Ajan Ã¶ÄŸrenir.
                state = next_state # Durumu gÃ¼ncelle.
                total_reward += reward # Toplam Ã¶dÃ¼lÃ¼ gÃ¼ncelle.
                step_counter += 1
                if self.mode == "human": # EÄŸer 'human' modundaysa
                    self.state_update.emit() # ArayÃ¼zÃ¼ her adÄ±mda gÃ¼ncelle.
                    QThread.msleep(int(self.delay * 1000)) # Belirlenen sÃ¼re kadar bekle.
                elif self.mode == "fast" and step_counter % self.update_interval == 0: # EÄŸer 'fast' modundaysa ve belirli aralÄ±klarla # "ansi" -> "fast"
                    self.state_update.emit() # ArayÃ¼zÃ¼ gÃ¼ncelle.
            rewards_per_episode.append(total_reward) # BÃ¶lÃ¼m Ã¶dÃ¼lÃ¼nÃ¼ listeye ekle.
            steps_per_episode.append(self.env.steps) # BÃ¶lÃ¼m adÄ±m sayÄ±sÄ±nÄ± listeye ekle.
            self.agent.decay_epsilon() # Epsilon deÄŸerini azalt.
            self.state_update.emit() # ArayÃ¼zÃ¼ gÃ¼ncelle.
            self.progress.emit(episode+1, total_reward, self.env.steps, self.agent.epsilon) # Ä°lerleme sinyalini gÃ¶nder.
        self.finished.emit(rewards_per_episode, steps_per_episode) # EÄŸitim bitti sinyalini gÃ¶nder.
    def stop(self):
        # EÄŸitimi durdurmak iÃ§in kullanÄ±lÄ±r.
        self.running = False

# =====================
# Grid ve Bilgi Paneli (PyQt5)
# =====================
class GridWidget(QWidget): # OrtamÄ±n grid yapÄ±sÄ±nÄ± gÃ¶rselleÅŸtiren widget.
    def __init__(self, env, parent=None):
        super().__init__(parent)
        self.env = env # GÃ¶rselleÅŸtirilecek ortam.
        self.cell_size = 80 # Her bir grid hÃ¼cresinin piksel boyutu.
        self.setMinimumSize(env.grid_size * self.cell_size, env.grid_size * self.cell_size)
        # Renkler ve gÃ¶rsel ayarlar
        self.colors = {
            'background': Qt.white,
            'grid': Qt.lightGray,
            'drone': Qt.blue,
            'drone_landed': QColor(100, 100, 180), # Ä°niÅŸ yapmÄ±ÅŸ drone rengi.
            'cargo_depot': Qt.green, # Kargo deposu rengi.
            'delivery_point': Qt.red, # Teslimat noktasÄ± rengi.
            'cargo': Qt.green, # Kargo rengi.
            'shadow': QColor(100, 100, 100, 80) # Drone uÃ§arkenki gÃ¶lge rengi.
        }
    def paintEvent(self, event):
        # Grid ve tÃ¼m nesneleri Ã§iz
        # Bu fonksiyon, widget her yeniden Ã§izildiÄŸinde Ã§aÄŸrÄ±lÄ±r.
        painter = QPainter(self)
        painter.setRenderHint(QPainter.Antialiasing) # Daha pÃ¼rÃ¼zsÃ¼z Ã§izimler iÃ§in.
        painter.fillRect(self.rect(), self.colors['background']) # Arka planÄ± boya.
        # Ortalamak iÃ§in offset hesapla
        grid_pixel_size = self.env.grid_size * self.cell_size
        x_offset = (self.width() - grid_pixel_size) // 2
        y_offset = (self.height() - grid_pixel_size) // 2
        # Grid Ã§izgileri
        painter.setPen(QPen(self.colors['grid'], 1))
        for i in range(self.env.grid_size + 1):
            painter.drawLine(x_offset, y_offset + i * self.cell_size, x_offset + self.env.grid_size * self.cell_size, y_offset + i * self.cell_size)
            painter.drawLine(x_offset + i * self.cell_size, y_offset, x_offset + i * self.cell_size, y_offset + self.env.grid_size * self.cell_size)
        # Kargo deposu Ã§izimi
        depot_x = x_offset + self.env.cargo_depot_pos[1] * self.cell_size + self.cell_size // 2
        depot_y = y_offset + self.env.cargo_depot_pos[0] * self.cell_size + self.cell_size // 2
        painter.setBrush(QBrush(self.colors['cargo_depot']))
        painter.setPen(Qt.NoPen) # Kenar Ã§izgisi olmasÄ±n.
        painter.drawEllipse(depot_x - self.cell_size // 3, depot_y - self.cell_size // 3, 2 * self.cell_size // 3, 2 * self.cell_size // 3)
        # Teslimat noktalarÄ± Ã§izimi
        painter.setBrush(QBrush(self.colors['delivery_point']))
        for i, point in enumerate(self.env.delivery_points):
            if i < len(self.env.delivered) and not self.env.delivered[i]: # HenÃ¼z teslim edilmemiÅŸse Ã§iz.
                x = x_offset + point[1] * self.cell_size + self.cell_size // 2
                y = y_offset + point[0] * self.cell_size + self.cell_size // 2
                painter.drawEllipse(x - self.cell_size // 4, y - self.cell_size // 4, self.cell_size // 2, self.cell_size // 2)
                painter.setPen(Qt.black) # Teslimat noktasÄ± numarasÄ±nÄ± yazmak iÃ§in.
                painter.setFont(QFont('Arial', 10))
                painter.drawText(x - 5, y + 5, str(i + 1)) # Teslimat noktasÄ± numarasÄ±nÄ± yaz.
                painter.setPen(Qt.NoPen)
        # Drone Ã§izimi
        drone_x = x_offset + self.env.drone_pos[1] * self.cell_size + self.cell_size // 2
        drone_y = y_offset + self.env.drone_pos[0] * self.cell_size + self.cell_size // 2
        if self.env.is_flying: # Drone uÃ§uyorsa
            # GÃ¶lge efekti
            painter.setBrush(QBrush(self.colors['shadow']))
            painter.drawEllipse(drone_x - self.cell_size // 6, drone_y + self.cell_size // 4, self.cell_size // 3, self.cell_size // 8)
            height_offset = 0 # YÃ¼kseklik ofseti (animasyon iÃ§in).
            if self.env.landing_state == "taking_off": # KalkÄ±ÅŸ animasyonu
                height_offset = -5 * self.env.landing_animation_step
            elif self.env.landing_state == "landing": # Ä°niÅŸ animasyonu
                height_offset = -15 + 5 * self.env.landing_animation_step
            elif self.env.landing_state == "flying": # Normal uÃ§uÅŸ
                height_offset = -15
            drone_y += height_offset # Drone'un dikey konumunu ayarla.
            painter.setBrush(QBrush(self.colors['drone'])) # UÃ§an drone rengi.
        else: # Drone yerdeyse
            painter.setBrush(QBrush(self.colors['drone_landed'])) # Ä°niÅŸ yapmÄ±ÅŸ drone rengi.
        # Drone gÃ¶vdesi
        painter.drawEllipse(drone_x - self.cell_size // 4, drone_y - self.cell_size // 4, self.cell_size // 2, self.cell_size // 2)
        # Pervaneler
        propeller_size = self.cell_size // 8
        if self.env.is_flying: # UÃ§arken pervaneler daha bÃ¼yÃ¼k gÃ¶rÃ¼nebilir.
            propeller_size = self.cell_size // 6
        painter.setBrush(QBrush(Qt.black)) # Pervane rengi.
        # Sol Ã¼st
        painter.drawEllipse(drone_x - propeller_size - propeller_size//2, drone_y - propeller_size - propeller_size//2, propeller_size, propeller_size)
        # SaÄŸ Ã¼st
        painter.drawEllipse(drone_x + propeller_size - propeller_size//2, drone_y - propeller_size - propeller_size//2, propeller_size, propeller_size)
        # Sol alt
        painter.drawEllipse(drone_x - propeller_size - propeller_size//2, drone_y + propeller_size - propeller_size//2, propeller_size, propeller_size)
        # SaÄŸ alt
        painter.drawEllipse(drone_x + propeller_size - propeller_size//2, drone_y + propeller_size - propeller_size//2, propeller_size, propeller_size)
        # Kargo Ã§izimi
        if self.env.has_cargo: # EÄŸer drone kargo taÅŸÄ±yorsa
            painter.setBrush(QBrush(self.colors['cargo'])) # Kargo rengi.
            painter.drawRect(drone_x - self.cell_size // 8, drone_y - self.cell_size // 8, self.cell_size // 4, self.cell_size // 4)
        # Batarya gÃ¶stergesi
        painter.setPen(Qt.black)
        painter.setFont(QFont('Arial', 10))
        painter.drawText(drone_x - 20, drone_y - 30, f"ğŸ”‹: {self.env.battery}%") # Drone Ã¼zerinde batarya seviyesini gÃ¶ster.

class InfoPanelWidget(QWidget): # Ortam ve eÄŸitim bilgilerini gÃ¶steren widget.
    def __init__(self, env, parent=None):
        super().__init__(parent)
        self.env = env # Bilgileri gÃ¶sterilecek ortam.
        layout = QVBoxLayout()
        # Bilgi paneli iÃ§in GroupBox
        info_group = QGroupBox("â„¹ï¸ Durum Bilgileri")
        info_layout = QVBoxLayout()
        self.battery_label = QLabel() # Batarya bilgisi etiketi.
        self.cargo_label = QLabel() # Kargo durumu etiketi.
        self.delivery_label = QLabel() # Teslimat durumu etiketi.
        self.steps_label = QLabel() # AdÄ±m sayÄ±sÄ± etiketi.
        self.reward_label = QLabel()  # Son Ã¶dÃ¼l etiketi
        self.total_reward_label = QLabel()  # Toplam Ã¶dÃ¼l etiketi
        self.last_action_label = QLabel()  # Son aksiyon etiketi
        self.training_progress_label = QLabel()  # EÄŸitim ilerlemesi etiketi
        info_layout.addWidget(self.battery_label)
        info_layout.addWidget(self.cargo_label)
        info_layout.addWidget(self.delivery_label)
        info_layout.addWidget(self.steps_label)
        info_layout.addWidget(self.reward_label)  # Son Ã¶dÃ¼l panelde gÃ¶ster
        info_layout.addWidget(self.total_reward_label)  # Toplam Ã¶dÃ¼l panelde gÃ¶ster
        info_layout.addWidget(self.last_action_label)  # Son aksiyon panelde gÃ¶ster
        info_layout.addWidget(self.training_progress_label)  # Durum bilgisine eklendi
        info_group.setLayout(info_layout)
        self.status_label = QLabel() # Genel durum mesajlarÄ± iÃ§in etiket.
        # Ana layout
        layout.addWidget(info_group)
        layout.addWidget(self.status_label)
        layout.addStretch() # Widget'larÄ± yukarÄ±ya iter.
        self.setLayout(layout)
        self.update_info() # Bilgileri ilk kez gÃ¼ncelle.
    def update_info(self):
        # Paneldeki tÃ¼m bilgileri gÃ¼nceller.
        self.battery_label.setText(f"ğŸ”‹ Batarya: %{self.env.battery}")
        # Kargo etiketi: taÅŸÄ±nÄ±yorsa kalÄ±n yeÅŸil
        if self.env.has_cargo:
            self.cargo_label.setText("ğŸ“¦ Kargo: <span style='color:#1ca81c; font-weight:bold;'>TaÅŸÄ±nÄ±yor</span>")
            self.cargo_label.setTextFormat(Qt.RichText) # HTML formatÄ±nda metin.
            self.cargo_label.setStyleSheet("")
        else:
            self.cargo_label.setText("ğŸ“¦ Kargo: Yok")
            self.cargo_label.setTextFormat(Qt.AutoText)
            self.cargo_label.setStyleSheet("")
        # Teslimat etiketi: teslim edilen sayÄ± yeÅŸil ve kalÄ±n
        delivered_count = sum(self.env.delivered) # Teslim edilen paket sayÄ±sÄ±.
        total = len(self.env.delivery_points) # Toplam teslimat noktasÄ± sayÄ±sÄ±.
        if delivered_count > 0:
            self.delivery_label.setText(f"ğŸ¯ Teslimatlar: <span style='color:#1ca81c; font-weight:bold;'>{delivered_count}</span>/{total}")
            self.delivery_label.setTextFormat(Qt.RichText)
        else:
            self.delivery_label.setText(f"ğŸ¯ Teslimatlar: 0/{total}")
            self.delivery_label.setTextFormat(Qt.AutoText)
        self.steps_label.setText(f"ğŸ‘£ AdÄ±m: {self.env.steps}")
        self.reward_label.setText(f"ğŸ… Son Ã–dÃ¼l: {self.env.last_reward:.2f}")  # Son Ã¶dÃ¼l gÃ¶sterimi
        self.total_reward_label.setText(f"ğŸ¥‡ Toplam Ã–dÃ¼l: {self.env.total_reward:.2f}")  # Toplam Ã¶dÃ¼l gÃ¶sterimi
        # Son aksiyon bilgisini grup kutusunda gÃ¶ster
        if hasattr(self.env, 'last_action_info') and self.env.last_action_info:
            self.last_action_label.setText(f"ğŸ”„ Son Aksiyon: {self.env.last_action_info}")
        else:
            self.last_action_label.setText("ğŸ”„ Son Aksiyon: -")
        # EÄŸitim ilerlemesi sadece eÄŸitim sÄ±rasÄ±nda gÃ¶sterilecek, aksi halde gizle
        if not self.training_progress_label.text(): # EÄŸer eÄŸitim ilerleme metni boÅŸsa
            self.training_progress_label.setVisible(False) # Etiketi gizle.
        else:
            self.training_progress_label.setVisible(True) # Etiketi gÃ¶ster.
    def set_status(self, status):
        # Genel durum mesajÄ±nÄ± ayarlar.
        self.status_label.setText(status)
    def set_training_progress(self, episode, total_episodes, reward, steps, epsilon):
        # EÄŸitim ilerleme bilgisini ayarlar.
        self.training_progress_label.setText(f"ğŸ“ˆ Episode: {episode}/{total_episodes} | Ã–dÃ¼l: {reward:.2f} | AdÄ±m: {steps} | Epsilon: {epsilon:.4f}")
        self.training_progress_label.setVisible(True) # Etiketi gÃ¶rÃ¼nÃ¼r yap.
    def clear_training_progress(self):
        # EÄŸitim ilerleme bilgisini temizler ve gizler.
        self.training_progress_label.setText("")
        self.training_progress_label.setVisible(False)

# =====================
# Ana PyQt5 ArayÃ¼zÃ¼
# =====================
class DroneDeliverySimulator(QMainWindow): # Ana uygulama penceresi.
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Paket DaÄŸÄ±tÄ±m DronlarÄ± SimÃ¼latÃ¶rÃ¼ - Q-Learning") # Pencere baÅŸlÄ±ÄŸÄ±.

        # Emoji ikonu oluÅŸturma
        emoji = "ğŸš"
        pixmap = QPixmap(64, 64) # Ä°kon boyutu
        pixmap.fill(Qt.transparent) # Åeffaf arka plan
        painter = QPainter(pixmap)
        font = QFont()
        font.setPointSize(48) # Emoji boyutu
        painter.setFont(font)
        painter.drawText(pixmap.rect(), Qt.AlignCenter, emoji)
        painter.end()
        self.setWindowIcon(QIcon(pixmap))

        self.resize(1200, 700) # Pencere boyutu.
        self.grid_size = 5 # BaÅŸlangÄ±Ã§ grid boyutu.
        self.env = DroneDeliveryEnv(grid_size=self.grid_size) # OrtamÄ± oluÅŸtur.
        self.agent = QLearningAgent(self.env) # AjanÄ± oluÅŸtur.
        self.training_thread = None # EÄŸitim thread'i baÅŸlangÄ±Ã§ta yok.
        self.sim_speed = 50  # AI ile oyna hÄ±z (ms).
        # --- Ana Layout ---
        central_widget = QWidget()
        self.setCentralWidget(central_widget) # Ana widget'Ä± ayarla.
        main_layout = QHBoxLayout(central_widget) # Ana layout (yatay).
        # --- Sol Panel: Parametreler ve Kontroller ---
        left_panel = QWidget()
        left_layout = QVBoxLayout(left_panel) # Sol panel layout'u (dikey).
        # Grid boyutu
        grid_group = QGroupBox("ğŸ—ºï¸ Grid AyarlarÄ±")
        grid_layout = QHBoxLayout()
        grid_layout.addWidget(QLabel("Grid Boyutu:"))
        self.grid_size_spin = QSpinBox() # Grid boyutu iÃ§in spin box.
        self.grid_size_spin.setRange(3, 7) # Min ve max grid boyutu.
        self.grid_size_spin.setValue(self.grid_size)
        self.grid_size_spin.valueChanged.connect(self.update_grid_size) # DeÄŸer deÄŸiÅŸtiÄŸinde fonksiyon Ã§aÄŸÄ±r.
        grid_layout.addWidget(self.grid_size_spin)
        grid_group.setLayout(grid_layout)
        left_layout.addWidget(grid_group)
        # Q-Learning parametreleri
        ql_group = QGroupBox("ğŸ¤– Q-Learning Parametreleri")
        ql_layout = QGridLayout() # Parametreleri grid iÃ§inde dÃ¼zenle.
        ql_layout.addWidget(QLabel("Alpha (Ã–ÄŸrenme OranÄ±):"), 0, 0)
        self.alpha_spin = QDoubleSpinBox(); self.alpha_spin.setRange(0.01, 1.0); self.alpha_spin.setSingleStep(0.01); self.alpha_spin.setValue(self.agent.alpha)
        ql_layout.addWidget(self.alpha_spin, 0, 1)
        ql_layout.addWidget(QLabel("Gamma:"), 1, 0)
        self.gamma_spin = QDoubleSpinBox(); self.gamma_spin.setRange(0.1, 0.999); self.gamma_spin.setSingleStep(0.01); self.gamma_spin.setValue(self.agent.gamma)
        ql_layout.addWidget(self.gamma_spin, 1, 1)
        ql_layout.addWidget(QLabel("Epsilon (KeÅŸif OranÄ±):"), 2, 0)
        self.epsilon_spin = QDoubleSpinBox(); self.epsilon_spin.setRange(0.1, 1.0); self.epsilon_spin.setSingleStep(0.1); self.epsilon_spin.setValue(self.agent.epsilon)
        ql_layout.addWidget(self.epsilon_spin, 2, 1)
        ql_layout.addWidget(QLabel("Epsilon Decay:"), 3, 0)
        self.epsilon_decay_spin = QDoubleSpinBox(); self.epsilon_decay_spin.setRange(0.9, 0.9999); self.epsilon_decay_spin.setSingleStep(0.001); self.epsilon_decay_spin.setValue(self.agent.epsilon_decay)
        ql_layout.addWidget(self.epsilon_decay_spin, 3, 1)
        ql_layout.addWidget(QLabel("Min Epsilon:"), 4, 0)
        self.min_epsilon_spin = QDoubleSpinBox(); self.min_epsilon_spin.setRange(0.01, 0.5); self.min_epsilon_spin.setSingleStep(0.01); self.min_epsilon_spin.setValue(self.agent.min_epsilon)
        ql_layout.addWidget(self.min_epsilon_spin, 4, 1)
        ql_layout.addWidget(QLabel("EÄŸitim Episodes:"), 5, 0)
        self.episodes_spin = QSpinBox(); self.episodes_spin.setRange(100, 100000); self.episodes_spin.setSingleStep(100); self.episodes_spin.setValue(5000) # EÄŸitim bÃ¶lÃ¼mÃ¼ sayÄ±sÄ±.
        ql_layout.addWidget(self.episodes_spin, 5, 1)
        ql_group.setLayout(ql_layout)
        left_layout.addWidget(ql_group)
        # EÄŸitim hÄ±zÄ± (mod ve delay)
        speed_group = QGroupBox("âš¡ EÄŸitim/SimÃ¼lasyon HÄ±zÄ±")
        speed_layout = QGridLayout()
        speed_layout.addWidget(QLabel("EÄŸitim Modu:"), 0, 0)
        self.training_mode_combo = QComboBox(); 
        self.training_mode_combo.addItems(["Fast Mode", "Human Mode"]) # EÄŸitim modu seÃ§enekleri.
        speed_layout.addWidget(self.training_mode_combo, 0, 1)
        speed_layout.addWidget(QLabel("EÄŸitim HÄ±zÄ± (ms):"), 1, 0) # CanlÄ± mod iÃ§in eÄŸitim hÄ±zÄ±.
        speed_slider_row = QHBoxLayout()
        speed_slider_row.addWidget(QLabel("ğŸï¸"))
        self.training_speed_slider = QSlider(Qt.Horizontal); self.training_speed_slider.setRange(10, 1000); self.training_speed_slider.setValue(100) # HÄ±z ayarÄ± iÃ§in slider.
        speed_slider_row.addWidget(self.training_speed_slider)
        speed_slider_row.addWidget(QLabel("ğŸ¢"))
        speed_layout.addLayout(speed_slider_row, 1, 1)
        speed_layout.addWidget(QLabel("SimÃ¼lasyon HÄ±zÄ± (ms):"), 2, 0) # AI ile oynama hÄ±zÄ±.
        sim_slider_row = QHBoxLayout()
        sim_slider_row.addWidget(QLabel("ğŸï¸"))
        self.sim_speed_slider = QSlider(Qt.Horizontal); self.sim_speed_slider.setRange(10, 1000); self.sim_speed_slider.setValue(self.sim_speed)
        self.sim_speed_slider.valueChanged.connect(self.update_sim_speed) # DeÄŸer deÄŸiÅŸtiÄŸinde fonksiyon Ã§aÄŸÄ±r.
        sim_slider_row.addWidget(self.sim_speed_slider)
        sim_slider_row.addWidget(QLabel("ğŸ¢"))
        speed_layout.addLayout(sim_slider_row, 2, 1)
        speed_group.setLayout(speed_layout)
        left_layout.addWidget(speed_group)
        # EÄŸitim kontrolleri
        training_group = QGroupBox("ğŸ“ EÄŸitim")
        training_layout = QVBoxLayout()
        self.train_button = QPushButton("ğŸš€ EÄŸitimi BaÅŸlat"); self.train_button.clicked.connect(self.start_training) # EÄŸitimi baÅŸlat butonu.
        self.stop_button = QPushButton("â¹ï¸ EÄŸitimi Durdur"); self.stop_button.clicked.connect(self.stop_training); self.stop_button.setEnabled(False) # EÄŸitimi durdur butonu (baÅŸlangÄ±Ã§ta pasif).
        self.save_button = QPushButton("ğŸ’¾ Modeli Kaydet"); self.save_button.clicked.connect(self.save_model); self.save_button.setEnabled(False) # Modeli kaydet butonu (baÅŸlangÄ±Ã§ta pasif).
        self.load_button = QPushButton("ğŸ“‚ Modeli YÃ¼kle"); self.load_button.clicked.connect(self.load_model) # Modeli yÃ¼kle butonu.
        training_layout.addWidget(self.train_button)
        training_layout.addWidget(self.stop_button)
        training_layout.addWidget(self.save_button)
        training_layout.addWidget(self.load_button)
        training_group.setLayout(training_layout)
        left_layout.addWidget(training_group)
        # Oyun kontrolleri
        game_group = QGroupBox("ğŸ® Oyun Kontrolleri")
        game_layout = QVBoxLayout()
        self.ai_button = QPushButton("ğŸ¤– AI ile Oyna"); self.ai_button.clicked.connect(self.play_with_ai); self.ai_button.setEnabled(False) # AI ile oyna butonu (baÅŸlangÄ±Ã§ta pasif).
        self.human_button = QPushButton("ğŸ§‘â€ğŸ’» Ä°nsan Modu (Manuel Oyna)"); self.human_button.clicked.connect(self.play_human_mode) # Manuel oynama butonu.
        self.stop_game_button = QPushButton("â¹ï¸ Oyunu Durdur"); self.stop_game_button.clicked.connect(self.stop_game); self.stop_game_button.setEnabled(False) # Oyunu durdur butonu (baÅŸlangÄ±Ã§ta pasif).
        self.reset_button = QPushButton("ğŸ”„ SÄ±fÄ±rla"); self.reset_button.clicked.connect(self.reset_env) # OrtamÄ± sÄ±fÄ±rla butonu.
        game_layout.addWidget(self.ai_button)
        game_layout.addWidget(self.human_button)
        game_layout.addWidget(self.stop_game_button)
        game_layout.addWidget(self.reset_button)
        game_group.setLayout(game_layout)
        left_layout.addWidget(game_group)
        left_layout.addStretch() # Sol paneli yukarÄ± iter.
        # --- SaÄŸ Panel: Grid ve Bilgi Paneli ---
        right_panel = QWidget()
        right_layout = QVBoxLayout(right_panel) # SaÄŸ panel layout'u (dikey).
        self.grid_widget = GridWidget(self.env) # Grid widget'Ä±nÄ± oluÅŸtur.
        self.info_panel = InfoPanelWidget(self.env) # Bilgi paneli widget'Ä±nÄ± oluÅŸtur.
        right_layout.addWidget(self.grid_widget, 7) # Grid widget'Ä±nÄ± ekle (daha fazla yer kaplasÄ±n).
        right_layout.addWidget(self.info_panel, 3) # Bilgi panelini ekle.
        # --- LayoutlarÄ± birleÅŸtir ---
        main_layout.addWidget(left_panel, 1) # Sol paneli ana layout'a ekle (daha az yer kaplasÄ±n).
        main_layout.addWidget(right_panel, 4) # SaÄŸ paneli ana layout'a ekle (daha fazla yer kaplasÄ±n).
        # --- Timer ---
        self.game_timer = QTimer(); self.game_timer.timeout.connect(self.update_game) # Oyun dÃ¶ngÃ¼sÃ¼ iÃ§in timer.
        self.game_mode = None # Oyun modu (ai, human, None).
        self.model_trained = False # Modelin eÄŸitilip eÄŸitilmediÄŸi.
        self.model_loaded = False # Modelin yÃ¼klenip yÃ¼klenmediÄŸi.
        self.update_ui() # ArayÃ¼zÃ¼ ilk kez gÃ¼ncelle.
        self.statusBar().showMessage("HazÄ±r - EÄŸitim veya AI ile oynamak iÃ§in modeli eÄŸitin/yÃ¼kleyin.") # Durum Ã§ubuÄŸu mesajÄ±.

        # GitHub linki
        github_link_label = QLabel('Coded by <a href="https://github.com/FerhatAkalan">Ferhat Akalan</a>')
        github_link_label.setOpenExternalLinks(True)
        self.statusBar().addPermanentWidget(github_link_label)

    def set_params_enabled(self, enabled: bool):
        # Q-Learning ve grid parametrelerinin aktif/pasif durumunu ayarlar.
        self.grid_size_spin.setEnabled(enabled)
        self.alpha_spin.setEnabled(enabled)
        self.gamma_spin.setEnabled(enabled)
        self.epsilon_spin.setEnabled(enabled)
        self.epsilon_decay_spin.setEnabled(enabled)
        self.min_epsilon_spin.setEnabled(enabled)
        self.episodes_spin.setEnabled(enabled)
        self.training_mode_combo.setEnabled(enabled)
        self.training_speed_slider.setEnabled(enabled)
        self.sim_speed_slider.setEnabled(enabled)

    def set_game_buttons_enabled(self, enabled: bool):
        # Oyunla ilgili butonlarÄ±n aktif/pasif durumunu ayarlar.
        self.train_button.setEnabled(enabled)
        self.ai_button.setEnabled(enabled and (self.model_trained or self.model_loaded)) # AI butonu model varsa aktif olur.
        self.human_button.setEnabled(enabled)
        self.reset_button.setEnabled(enabled)
        self.save_button.setEnabled(enabled and self.model_trained) # Kaydet butonu model eÄŸitildiyse aktif olur.
        self.load_button.setEnabled(enabled)

    def update_grid_size(self):
        # Grid boyutu deÄŸiÅŸtiÄŸinde Ã§aÄŸrÄ±lÄ±r.
        self.grid_size = self.grid_size_spin.value()
        self.reset_env() # OrtamÄ± yeni grid boyutuyla sÄ±fÄ±rla.
    def update_sim_speed(self):
        # SimÃ¼lasyon hÄ±zÄ± (AI ile oynama hÄ±zÄ±) deÄŸiÅŸtiÄŸinde Ã§aÄŸrÄ±lÄ±r.
        self.sim_speed = self.sim_speed_slider.value()
        if self.game_timer.isActive(): # EÄŸer oyun zamanlayÄ±cÄ±sÄ± aktifse
            self.game_timer.setInterval(self.sim_speed) # ZamanlayÄ±cÄ±nÄ±n aralÄ±ÄŸÄ±nÄ± gÃ¼ncelle.
    def reset_env(self):
        # OrtamÄ± ve ajanÄ± sÄ±fÄ±rlar.
        if self.game_timer.isActive(): # EÄŸer oyun zamanlayÄ±cÄ±sÄ± aktifse durdur.
            self.game_timer.stop(); self.game_mode = None
        self.env = DroneDeliveryEnv(grid_size=self.grid_size) # Yeni ortam oluÅŸtur.
        self.agent = QLearningAgent(self.env) # Yeni ajan oluÅŸtur (Q-tablosu sÄ±fÄ±rlanÄ±r).
        self.grid_widget.env = self.env # Grid widget'Ä±nÄ±n ortamÄ±nÄ± gÃ¼ncelle.
        self.info_panel.env = self.env # Bilgi panelinin ortamÄ±nÄ± gÃ¼ncelle.
        self.model_trained = False # Model eÄŸitilmedi olarak iÅŸaretle.
        self.model_loaded = False # Model yÃ¼klenmedi olarak iÅŸaretle.
        self.update_ui() # ArayÃ¼zÃ¼ gÃ¼ncelle.
        self.set_game_buttons_enabled(True) # ButonlarÄ± aktif et.
        self.set_params_enabled(True) # Parametreleri aktif et.
        self.info_panel.clear_training_progress() # EÄŸitim ilerlemesini temizle.
        self.statusBar().showMessage("Ortam sÄ±fÄ±rlandÄ±")
    def update_ui(self):
        # ArayÃ¼zdeki grid ve bilgi panelini gÃ¼nceller.
        self.grid_widget.update(); self.info_panel.update_info()
    def update_game(self):
        # AI ile oynama modunda oyunun bir adÄ±mÄ±nÄ± gÃ¼nceller.
        if self.game_mode == 'ai':
            if not hasattr(self, 'ai_episode_count'):
                self.ai_episode_count = 1
                self.ai_total_reward = 0
            if not hasattr(self, 'ai_episode_running'):
                self.ai_episode_running = True
            if not self.ai_episode_running:
                return  # Yeni episode baÅŸlatÄ±lana kadar bekle
            state = self.env.get_state()
            action = self.agent.select_action(state, training=False)
            next_state, reward, done, info = self.env.step(action)
            self.ai_total_reward += reward
            self.update_ui()
            if done:
                delivered = sum(self.env.delivered)
                # Bilgi panelinde ve durum Ã§ubuÄŸunda bÃ¶lÃ¼m sonucunu gÃ¶ster.
                self.info_panel.set_status(f"ğŸ¤–AI Episode: {self.ai_episode_count} | ğŸ¯Teslimat: {delivered}/{len(self.env.delivery_points)} | ğŸ”‹Batarya: %{self.env.battery} | ğŸ¥‡Skor: {self.ai_total_reward:.2f}")
                self.ai_episode_running = False  # Episode bitti, yeni episode baÅŸlatÄ±lana kadar adÄ±m atma
                self.ai_episode_count += 1
                self.ai_total_reward = 0
                QTimer.singleShot(200, self.env.reset)
                QTimer.singleShot(350, self.start_next_ai_episode)
                QTimer.singleShot(400, self.update_ui)

    def start_next_ai_episode(self):
        # Yeni AI episode baÅŸlatmak iÃ§in flag'i tekrar aktif et
        self.ai_episode_running = True

    def start_training(self):
        # EÄŸitimi baÅŸlatÄ±r.
        # Ajan parametrelerini arayÃ¼zdeki deÄŸerlerle gÃ¼nceller.
        self.agent.alpha = self.alpha_spin.value()
        self.agent.gamma = self.gamma_spin.value()
        self.agent.epsilon = self.epsilon_spin.value()
        self.agent.epsilon_decay = self.epsilon_decay_spin.value()
        self.agent.min_epsilon = self.min_epsilon_spin.value()
        episodes = self.episodes_spin.value() # EÄŸitim bÃ¶lÃ¼mÃ¼ sayÄ±sÄ±nÄ± al.
        mode_text = self.training_mode_combo.currentText() # SeÃ§ilen eÄŸitim modunu al.
        training_mode = "human" if "human" in mode_text.lower() else "ansi" # EÄŸitim modunu belirle.
        delay = self.training_speed_slider.value() / 1000.0 if hasattr(self, 'training_speed_slider') else 0.1 # CanlÄ± mod iÃ§in gecikme.
        
        self.info_panel.clear_training_progress()  # EÄŸitim baÅŸÄ±nda ilerleme bilgisini temizle.
        # Buton ve parametrelerin durumunu ayarla (eÄŸitim sÄ±rasÄ±nda Ã§oÄŸu pasif olur).
        self.train_button.setEnabled(False)
        self.stop_button.setEnabled(True)
        self.stop_game_button.setEnabled(False)
        self.set_game_buttons_enabled(False)
        self.set_params_enabled(False)
        # ---
        self.env.reset() # OrtamÄ± sÄ±fÄ±rla.
        self.training_rewards = []; self.training_steps = [] # Ã–dÃ¼l ve adÄ±m listelerini sÄ±fÄ±rla.
        # EÄŸitim thread'ini oluÅŸtur ve baÅŸlat.
        self.training_thread = TrainingThread(self.env, self.agent, episodes, mode=training_mode, delay=delay)
        self.training_thread.progress.connect(self.update_training_progress) # Ä°lerleme sinyaline baÄŸlan.
        self.training_thread.finished.connect(self.training_finished) # BitiÅŸ sinyaline baÄŸlan.
        self.training_thread.state_update.connect(self.update_training_visualization) # Durum gÃ¼ncelleme sinyaline baÄŸlan.
        self.training_thread.start() # Thread'i baÅŸlat.
        self.info_panel.set_status("EÄŸitim devam ediyor...")
        self.statusBar().showMessage(f"EÄŸitim baÅŸladÄ±. Toplam episode: {episodes}")
    def update_training_visualization(self):
        # EÄŸitim sÄ±rasÄ±nda arayÃ¼zÃ¼ gÃ¼nceller (Ã¶zellikle canlÄ± modda).
        self.update_ui()
    def update_training_progress(self, episode, reward, steps, epsilon):
        # EÄŸitim ilerlemesini alÄ±r ve arayÃ¼zde gÃ¶sterir.
        self.training_rewards.append(reward)
        self.training_steps.append(steps)
        # EÄŸitim bilgisi sadece eÄŸitim sÄ±rasÄ±nda gÃ¶sterilecek
        self.info_panel.set_training_progress(episode, self.episodes_spin.value(), reward, steps, epsilon)
        self.info_panel.update_info() # Bilgi panelini gÃ¼ncelle.
        self.update_ui() # Genel arayÃ¼zÃ¼ gÃ¼ncelle.
    def training_finished(self, rewards, steps):
        # EÄŸitim bittiÄŸinde Ã§aÄŸrÄ±lÄ±r.
        # Buton ve parametrelerin durumunu eski haline getirir.
        self.train_button.setEnabled(True)
        self.stop_button.setEnabled(False)
        self.stop_game_button.setEnabled(False)
        self.set_game_buttons_enabled(True)
        self.info_panel.clear_training_progress()  # EÄŸitim bitince eÄŸitim bilgisini gizle.
        # Son 100 bÃ¶lÃ¼mÃ¼n ortalama Ã¶dÃ¼l ve adÄ±m sayÄ±sÄ±nÄ± hesapla.
        avg_reward = sum(rewards[-100:]) / min(100, len(rewards)) if rewards else 0
        avg_steps = sum(steps[-100:]) / min(100, len(steps)) if steps else 0
        result_message = f"EÄŸitim tamamlandÄ±!\n\nToplam episode: {len(rewards)}\nSon 100 episode ortalama Ã¶dÃ¼l: {avg_reward:.2f}\nSon 100 episode ortalama adÄ±m: {avg_steps:.2f}\n\nÅimdi 'AI ile Oyna' butonunu kullanarak eÄŸitilen modeli test edebilirsiniz."
        QMessageBox.information(self, "EÄŸitim TamamlandÄ±", result_message) # Bilgilendirme mesajÄ± gÃ¶ster.
        self.statusBar().showMessage(f"EÄŸitim tamamlandÄ±! Son 100 episode ortalama Ã¶dÃ¼l: {avg_reward:.2f}, adÄ±m: {avg_steps:.2f}")
        self.training_thread = None; self.model_trained = True # Model eÄŸitildi olarak iÅŸaretle.
        # self.training_status_label = QLabel("Model Durumu: EÄŸitildi"); self.training_status_label.setStyleSheet("color: green; font-weight: bold;") # Bu satÄ±r GUI'de bir yere eklenmeli.
        # EÄŸitim bitince parametreleri tekrar aktif et
        self.set_params_enabled(True)
        self.save_button.setEnabled(True) # Model eÄŸitildiÄŸi iÃ§in kaydet butonu aktif olur.
        self.ai_button.setEnabled(True) # Model eÄŸitildiÄŸi iÃ§in AI ile oyna butonu aktif olur.
    def stop_training(self):
        # EÄŸitimi durdurur.
        if self.training_thread:
            self.training_thread.stop() # EÄŸitim thread'ine durma sinyali gÃ¶nder.
            self.statusBar().showMessage("EÄŸitim durduruluyor...")
            self.set_params_enabled(True) # Parametreleri tekrar aktif et.
            # ButonlarÄ± da uygun ÅŸekilde ayarlamak gerekebilir.
            self.train_button.setEnabled(True)
            self.stop_button.setEnabled(False)
            self.set_game_buttons_enabled(True)
    def save_model(self):
        # EÄŸitilmiÅŸ Q-tablosunu kaydeder.
        save_dir = "models" # KayÄ±t dizini.
        if not os.path.exists(save_dir): os.makedirs(save_dir) # Dizin yoksa oluÅŸtur.
        # Dosya adÄ± iÃ§in zaman damgasÄ± ve grid boyutu kullanÄ±lÄ±r.
        timestamp = "qtable_" + str(self.grid_size) + "_" + str(random.randint(1000,9999)) + ".pkl"
        filename, _ = QFileDialog.getSaveFileName(self, "Q Tablosunu Kaydet", os.path.join(save_dir, timestamp), "Pickle Files (*.pkl);;All Files (*)") # KayÄ±t dialoÄŸu.
        if filename: # EÄŸer bir dosya adÄ± seÃ§ildiyse
            self.agent.save_q_table(filename) # AjanÄ±n Q-tablosunu kaydet.
            self.statusBar().showMessage(f"Q tablosu kaydedildi: {filename}")
    def load_model(self):
        # KaydedilmiÅŸ bir Q-tablosunu yÃ¼kler.
        filename, _ = QFileDialog.getOpenFileName(self, "Q Tablosu YÃ¼kle", "models" if os.path.exists("models") else ".", "Pickle Files (*.pkl);;All Files (*)") # YÃ¼kleme dialoÄŸu.
        if filename: # EÄŸer bir dosya seÃ§ildiyse
            try:
                self.agent.load_q_table(filename) # AjanÄ±n Q-tablosunu yÃ¼kle.
                self.model_loaded = True # Model yÃ¼klendi olarak iÅŸaretle.
                self.model_trained = True # YÃ¼klenen model eÄŸitilmiÅŸ sayÄ±lÄ±r.
                self.ai_button.setEnabled(True) # AI ile oyna butonunu aktif et.
                self.save_button.setEnabled(True) # YÃ¼klenen model kaydedilebilir.
                self.statusBar().showMessage(f"Model baÅŸarÄ±yla yÃ¼klendi: {filename}")
                # YÃ¼klenen modelin parametrelerini arayÃ¼ze yansÄ±tmak iyi bir fikir olabilir (eÄŸer saklanÄ±yorsa).
                # Ã–rneÄŸin, grid boyutu modelle uyumlu olmalÄ±.
                # Bu Ã¶rnekte Q-tablosu grid boyutundan baÄŸÄ±msÄ±z deÄŸil, bu yÃ¼zden grid boyutu da modelle birlikte dÃ¼ÅŸÃ¼nÃ¼lmeli.
                # EÄŸer model farklÄ± bir grid boyutu iÃ§in eÄŸitilmiÅŸse, kullanÄ±cÄ±ya bilgi verilebilir veya grid boyutu otomatik ayarlanabilir.
                # Åimdilik, yÃ¼klenen modelin mevcut grid boyutuyla uyumlu olduÄŸunu varsayÄ±yoruz.
            except Exception as e:
                QMessageBox.critical(self, "Model YÃ¼kleme HatasÄ±", f"Model yÃ¼klenirken bir hata oluÅŸtu: {e}")
                self.model_loaded = False
                self.model_trained = False
                self.ai_button.setEnabled(False)
                self.save_button.setEnabled(False)

    def play_with_ai(self):
        # EÄŸitilmiÅŸ veya yÃ¼klenmiÅŸ model ile AI'Ä±n oynamasÄ±nÄ± baÅŸlatÄ±r.
        if self.game_timer.isActive(): # EÄŸer zamanlayÄ±cÄ± zaten aktifse (yani AI oynuyorsa)
            self.stop_game(); return # Oyunu durdur.
        if not self.model_trained and not self.model_loaded: # EÄŸer model yoksa
            QMessageBox.warning(self, "Model Yok", "AI ile oynamak iÃ§in Ã¶nce modeli eÄŸitmeniz veya yÃ¼klemeniz gerekiyor.")
            return
        self.env.reset(); self.game_mode = 'ai'; self.info_panel.set_status("AI ile oynanÄ±yor...")
        self.info_panel.clear_training_progress()  # AI ile oyna baÅŸlarken eÄŸitim bilgisini gizle.
        self.ai_episode_count = 1; self.ai_total_reward = 0 # AI bÃ¶lÃ¼m sayacÄ±nÄ± ve Ã¶dÃ¼lÃ¼nÃ¼ sÄ±fÄ±rla.
        self.game_timer.start(self.sim_speed) # Oyun zamanlayÄ±cÄ±sÄ±nÄ± baÅŸlat.
        # Buton ve parametrelerin durumunu ayarla.
        self.stop_game_button.setEnabled(True)
        self.train_button.setEnabled(False)
        self.stop_button.setEnabled(False)
        self.set_game_buttons_enabled(False) # DiÄŸer oyun butonlarÄ±nÄ± pasif yap.
        self.ai_button.setEnabled(False) # AI ile oyna butonu zaten basÄ±ldÄ±ÄŸÄ± iÃ§in pasif.
        self.human_button.setEnabled(False)
        self.reset_button.setEnabled(False)
        self.load_button.setEnabled(False)
        self.save_button.setEnabled(False)
        self.set_params_enabled(False) # Parametreleri pasif yap.

    def play_human_mode(self):
        # KullanÄ±cÄ±nÄ±n manuel olarak oynamasÄ±nÄ± saÄŸlar.
        if self.game_timer.isActive(): # EÄŸer AI oynuyorsa
            self.stop_game(); return # Oyunu durdur.
        self.env.reset(); self.game_mode = 'human'; # OrtamÄ± sÄ±fÄ±rla ve oyun modunu 'human' yap.
        self.info_panel.set_status("Manuel mod: Hareket=WASD/Ok TuÅŸlarÄ±, UÃ§/Kalk/Ä°n=Space, Kargo=E"); # KullanÄ±cÄ±ya bilgi ver.
        self.info_panel.clear_training_progress()  # Ä°nsan modunda eÄŸitim bilgisini gizle.
        self.update_ui() # ArayÃ¼zÃ¼ gÃ¼ncelle.
        # Buton ve parametrelerin durumunu ayarla.
        self.stop_game_button.setEnabled(True)
        self.set_game_buttons_enabled(False) # DiÄŸer oyun butonlarÄ±nÄ± pasif yap.
        self.train_button.setEnabled(False)
        self.ai_button.setEnabled(False)
        self.human_button.setEnabled(False) # Manuel mod butonu zaten basÄ±ldÄ±ÄŸÄ± iÃ§in pasif.
        self.reset_button.setEnabled(False)
        self.load_button.setEnabled(False)
        self.save_button.setEnabled(False)
        self.set_params_enabled(False) # Parametreleri pasif yap.
        self.setFocus() # Klavye girdilerini almak iÃ§in pencereye odaklan.

    def keyPressEvent(self, event):
        # Klavye tuÅŸlarÄ±na basÄ±ldÄ±ÄŸÄ±nda Ã§aÄŸrÄ±lÄ±r (sadece manuel modda).
        if self.game_mode != 'human' or self.env.done: # EÄŸer manuel modda deÄŸilse veya bÃ¶lÃ¼m bittiyse bir ÅŸey yapma.
            return
        key = event.key() # BasÄ±lan tuÅŸu al.
        action = None # BaÅŸlangÄ±Ã§ta eylem yok.
        # WASD ve ok tuÅŸlarÄ± ile hareket
        if key in (Qt.Key_Down, Qt.Key_S): # AÅŸaÄŸÄ±
            action = 0
        elif key in (Qt.Key_Right, Qt.Key_D): # SaÄŸa
            action = 1
        elif key in (Qt.Key_Up, Qt.Key_W): # YukarÄ±
            action = 2
        elif key in (Qt.Key_Left, Qt.Key_A): # Sola
            action = 3
        # Kargo al/bÄ±rak: E
        elif key == Qt.Key_E:
            action = 4
        # Kalk/Ä°n: Space
        elif key == Qt.Key_Space:
            action = 5
        
        if action is not None: # EÄŸer geÃ§erli bir eylem tuÅŸuna basÄ±ldÄ±ysa
            _, _, done, info = self.env.step(action) # Eylemi uygula.
            self.update_ui() # ArayÃ¼zÃ¼ gÃ¼ncelle.
            if "action" in info and info["action"]: # EÄŸer eylemle ilgili bir mesaj varsa durum Ã§ubuÄŸunda gÃ¶ster.
                self.statusBar().showMessage(info["action"])
            if done: # EÄŸer bÃ¶lÃ¼m bittiyse
                self.info_panel.set_status("Oyun bitti! Manuel modda yeni oyun iÃ§in 'SÄ±fÄ±rla' veya 'Oyunu Durdur' kullanÄ±n.")
                # Oyun bittiÄŸinde bazÄ± butonlarÄ± tekrar aktif hale getirebiliriz.
                self.stop_game_button.setEnabled(False) # Oyunu durdur butonu pasif.
                self.reset_button.setEnabled(True) # SÄ±fÄ±rla butonu aktif.
                self.human_button.setEnabled(True) # Tekrar manuel oynamak iÃ§in.
                # DiÄŸer butonlar da duruma gÃ¶re ayarlanabilir.

    def stop_game(self):
        # AI veya manuel oyunu durdurur.
        if self.game_timer.isActive() or self.game_mode == 'human': # EÄŸer AI oynuyorsa veya manuel moddaysa
            self.game_timer.stop(); self.game_mode = None # ZamanlayÄ±cÄ±yÄ± durdur ve oyun modunu sÄ±fÄ±rla.
            self.info_panel.set_status("Oyun durduruldu.")
            self.info_panel.clear_training_progress()  # Oyun durunca eÄŸitim bilgisini gizle.
            self.statusBar().showMessage("Oyun durduruldu.")
            # Buton ve parametrelerin durumunu eski haline getir.
            self.stop_game_button.setEnabled(False)
            self.stop_button.setEnabled(False) # EÄŸitim durdurma butonu da pasif olmalÄ±.
            self.set_game_buttons_enabled(True) # Oyunla ilgili ana butonlarÄ± aktif et.
            self.set_params_enabled(True) # Parametreleri aktif et.
# =====================
# Ana Uygulama BaÅŸlatÄ±cÄ±
# =====================
if __name__ == "__main__":
    # PyQt5 uygulamasÄ±nÄ± baÅŸlatÄ±r.
    app = QApplication(sys.argv)
    window = DroneDeliverySimulator() # Ana pencereyi oluÅŸtur.
    window.show() # Pencereyi gÃ¶ster.
    sys.exit(app.exec_()) # Uygulama dÃ¶ngÃ¼sÃ¼nÃ¼ baÅŸlat ve Ã§Ä±kÄ±ÅŸta temizle.